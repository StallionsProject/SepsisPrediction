{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YERLTJjD4kk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejA41yIHEMMs"
   },
   "outputs": [],
   "source": [
    "### try autoregressive RNN model\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, n_inputs, n_hidden, n_rnnlayers, n_outputs):\n",
    "    super(RNN, self).__init__()\n",
    "    self.D = n_inputs\n",
    "    self.M = n_hidden\n",
    "    self.K = n_outputs\n",
    "    self.L = n_rnnlayers\n",
    "\n",
    "    self.rnn = nn.LSTM(\n",
    "        input_size=self.D,\n",
    "        hidden_size=self.M,\n",
    "        num_layers=self.L,\n",
    "        batch_first=True)\n",
    "    self.fc = nn.Linear(self.M, self.K)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    # initial hidden states\n",
    "    h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "    c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "\n",
    "    # get RNN unit output\n",
    "    out, _ = self.rnn(X, (h0, c0))\n",
    "\n",
    "    # we only want h(T) at the final time step\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "7zL5f7KWEcVw",
    "outputId": "4680ee2a-4550-4651-a833-3c86341da2bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbUEPOnZE1c9"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "def full_gd(model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            epochs=200):\n",
    "\n",
    "  # Stuff to store\n",
    "  train_losses = np.zeros(epochs)\n",
    "  test_losses = np.zeros(epochs)\n",
    "\n",
    "  for it in range(epochs):\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "      \n",
    "    # Backward and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = loss.item()\n",
    "\n",
    "    # Test loss\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    test_losses[it] = test_loss.item()\n",
    "      \n",
    "    if (it + 1) % 5 == 0:\n",
    "      print(f'Epoch {it+1}/{epochs}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "  \n",
    "  return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientunitstayid</th>\n",
       "      <th>observationoffset</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>respiration</th>\n",
       "      <th>systemicsystolic</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>wbcx1000</th>\n",
       "      <th>lactate</th>\n",
       "      <th>urineoutputbyweight</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143870</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>44.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>11.7</td>\n",
       "      <td>2.30671</td>\n",
       "      <td>3.870968</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143870</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>43.0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>112.500000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>11.7</td>\n",
       "      <td>2.30671</td>\n",
       "      <td>3.870968</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143870</td>\n",
       "      <td>12.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>42.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>11.7</td>\n",
       "      <td>2.30671</td>\n",
       "      <td>3.870968</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143870</td>\n",
       "      <td>17.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>41.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>11.7</td>\n",
       "      <td>2.30671</td>\n",
       "      <td>3.870968</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143870</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>41.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>11.7</td>\n",
       "      <td>2.30671</td>\n",
       "      <td>3.870968</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230089</th>\n",
       "      <td>3353113</td>\n",
       "      <td>2671.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>87.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>125.654566</td>\n",
       "      <td>0.57</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230090</th>\n",
       "      <td>3353113</td>\n",
       "      <td>2676.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>85.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>125.654566</td>\n",
       "      <td>0.57</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230091</th>\n",
       "      <td>3353113</td>\n",
       "      <td>2681.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>84.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>125.654566</td>\n",
       "      <td>0.57</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230092</th>\n",
       "      <td>3353113</td>\n",
       "      <td>2686.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>84.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>125.654566</td>\n",
       "      <td>0.57</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230093</th>\n",
       "      <td>3353113</td>\n",
       "      <td>2691.0</td>\n",
       "      <td>37.132325</td>\n",
       "      <td>84.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>125.654566</td>\n",
       "      <td>0.57</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1230094 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         patientunitstayid  observationoffset  temperature  heartrate  \\\n",
       "0                   143870                7.0    37.132325       44.0   \n",
       "1                   143870               10.0    37.132325       43.0   \n",
       "2                   143870               12.0    37.132325       42.0   \n",
       "3                   143870               17.0    37.132325       41.0   \n",
       "4                   143870               22.0    37.132325       41.0   \n",
       "...                    ...                ...          ...        ...   \n",
       "1230089            3353113             2671.0    37.132325       87.0   \n",
       "1230090            3353113             2676.0    37.132325       85.0   \n",
       "1230091            3353113             2681.0    37.132325       84.0   \n",
       "1230092            3353113             2686.0    37.132325       84.0   \n",
       "1230093            3353113             2691.0    37.132325       84.0   \n",
       "\n",
       "         respiration  systemicsystolic  creatinine  wbcx1000  lactate  \\\n",
       "0               86.0        111.000000        0.89      11.7  2.30671   \n",
       "1               80.5        112.500000        0.89      11.7  2.30671   \n",
       "2               75.0        114.000000        0.89      11.7  2.30671   \n",
       "3               78.0        113.000000        0.89      11.7  2.30671   \n",
       "4               73.0        113.000000        0.89      11.7  2.30671   \n",
       "...              ...               ...         ...       ...      ...   \n",
       "1230089         27.0        125.654566        0.57       5.0  1.80000   \n",
       "1230090         27.0        125.654566        0.57       5.0  1.80000   \n",
       "1230091         26.0        125.654566        0.57       5.0  1.80000   \n",
       "1230092         25.0        125.654566        0.57       5.0  1.80000   \n",
       "1230093         25.0        125.654566        0.57       5.0  1.80000   \n",
       "\n",
       "         urineoutputbyweight  diagnosis  \n",
       "0                   3.870968        0.0  \n",
       "1                   3.870968        0.0  \n",
       "2                   3.870968        0.0  \n",
       "3                   3.870968        0.0  \n",
       "4                   3.870968        0.0  \n",
       "...                      ...        ...  \n",
       "1230089             0.852273        0.0  \n",
       "1230090             0.852273        0.0  \n",
       "1230091             0.852273        0.0  \n",
       "1230092             0.852273        0.0  \n",
       "1230093             0.852273        0.0  \n",
       "\n",
       "[1230094 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../eICU/training/finalData.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bndx3QO5HuR9"
   },
   "outputs": [],
   "source": [
    "# Now turn the full data into numpy arrays\n",
    "\n",
    "# Not yet in the final \"X\" format!\n",
    "input_data = df[['temperature', 'heartrate', 'respiration', 'systemicsystolic', 'creatinine', 'wbcx1000', 'lactate', 'urineoutputbyweight']].values\n",
    "targets = df['diagnosis'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwdN0u8mH_GN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now make the actual data which will go into the neural network\n",
    "T = 10 # the number of time steps to look at to make a prediction for the next day\n",
    "D = input_data.shape[1]\n",
    "N = len(input_data) - T # (e.g. if T=10 and you have 11 data points then you'd only have 1 sample)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ivu7w3yIFcu"
   },
   "outputs": [],
   "source": [
    "# normalize the inputs\n",
    "Ntrain = len(input_data) * 2 // 3\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(input_data[:Ntrain + T - 1])\n",
    "input_data = scaler.transform(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3S0TJMWHIG68"
   },
   "outputs": [],
   "source": [
    "# Setup X_train and Y_train\n",
    "X_train = np.zeros((Ntrain, T, D))\n",
    "Y_train = np.zeros((Ntrain, 1))\n",
    "\n",
    "for t in range(Ntrain):\n",
    "  X_train[t, :, :] = input_data[t:t+T]\n",
    "  Y_train[t] = (targets[t+T] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5OPoVZnIIoq"
   },
   "outputs": [],
   "source": [
    "# Setup X_test and Y_test\n",
    "X_test = np.zeros((N - Ntrain, T, D))\n",
    "Y_test = np.zeros((N - Ntrain, 1))\n",
    "\n",
    "for u in range(N - Ntrain):\n",
    "  # u counts from 0...(N - Ntrain)\n",
    "  # t counts from Ntrain...N\n",
    "  t = u + Ntrain\n",
    "  X_test[u, :, :] = input_data[t:t+T]\n",
    "  Y_test[u] = (targets[t+T] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "TTXdPZW9ILDP",
    "outputId": "b5f4e8c1-7915-4e06-d5fd-abb3489b7b16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): LSTM(8, 5, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the RNN\n",
    "model = RNN(8, 5, 2, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkNDPzSgIPT8"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer - it's classification now!\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kthbTpgqIab5"
   },
   "outputs": [],
   "source": [
    "# Make inputs and targets\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "y_train = torch.from_numpy(Y_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_test = torch.from_numpy(Y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acXZJ9gOIhTI"
   },
   "outputs": [],
   "source": [
    "# move data to GPU\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "He5Z5-z_IjiZ",
    "outputId": "7cb40cb5-87ed-4624-dc91-51bd5c613ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000, Train Loss: 0.0159, Test Loss: 0.0161\n",
      "Epoch 10/1000, Train Loss: 0.0150, Test Loss: 0.0153\n",
      "Epoch 15/1000, Train Loss: 0.0142, Test Loss: 0.0145\n",
      "Epoch 20/1000, Train Loss: 0.0135, Test Loss: 0.0138\n",
      "Epoch 25/1000, Train Loss: 0.0128, Test Loss: 0.0132\n",
      "Epoch 30/1000, Train Loss: 0.0122, Test Loss: 0.0126\n",
      "Epoch 35/1000, Train Loss: 0.0117, Test Loss: 0.0121\n",
      "Epoch 40/1000, Train Loss: 0.0112, Test Loss: 0.0116\n",
      "Epoch 45/1000, Train Loss: 0.0107, Test Loss: 0.0111\n",
      "Epoch 50/1000, Train Loss: 0.0103, Test Loss: 0.0107\n",
      "Epoch 55/1000, Train Loss: 0.0099, Test Loss: 0.0103\n",
      "Epoch 60/1000, Train Loss: 0.0095, Test Loss: 0.0100\n",
      "Epoch 65/1000, Train Loss: 0.0092, Test Loss: 0.0096\n",
      "Epoch 70/1000, Train Loss: 0.0088, Test Loss: 0.0093\n",
      "Epoch 75/1000, Train Loss: 0.0085, Test Loss: 0.0090\n",
      "Epoch 80/1000, Train Loss: 0.0082, Test Loss: 0.0087\n",
      "Epoch 85/1000, Train Loss: 0.0080, Test Loss: 0.0085\n",
      "Epoch 90/1000, Train Loss: 0.0077, Test Loss: 0.0083\n",
      "Epoch 95/1000, Train Loss: 0.0075, Test Loss: 0.0080\n",
      "Epoch 100/1000, Train Loss: 0.0073, Test Loss: 0.0078\n",
      "Epoch 105/1000, Train Loss: 0.0071, Test Loss: 0.0076\n",
      "Epoch 110/1000, Train Loss: 0.0069, Test Loss: 0.0074\n",
      "Epoch 115/1000, Train Loss: 0.0067, Test Loss: 0.0072\n",
      "Epoch 120/1000, Train Loss: 0.0065, Test Loss: 0.0071\n",
      "Epoch 125/1000, Train Loss: 0.0064, Test Loss: 0.0069\n",
      "Epoch 130/1000, Train Loss: 0.0062, Test Loss: 0.0068\n",
      "Epoch 135/1000, Train Loss: 0.0060, Test Loss: 0.0066\n",
      "Epoch 140/1000, Train Loss: 0.0059, Test Loss: 0.0065\n",
      "Epoch 145/1000, Train Loss: 0.0058, Test Loss: 0.0063\n",
      "Epoch 150/1000, Train Loss: 0.0056, Test Loss: 0.0062\n",
      "Epoch 155/1000, Train Loss: 0.0055, Test Loss: 0.0061\n",
      "Epoch 160/1000, Train Loss: 0.0054, Test Loss: 0.0060\n",
      "Epoch 165/1000, Train Loss: 0.0053, Test Loss: 0.0059\n",
      "Epoch 170/1000, Train Loss: 0.0052, Test Loss: 0.0058\n",
      "Epoch 175/1000, Train Loss: 0.0051, Test Loss: 0.0057\n",
      "Epoch 180/1000, Train Loss: 0.0050, Test Loss: 0.0056\n",
      "Epoch 185/1000, Train Loss: 0.0049, Test Loss: 0.0055\n",
      "Epoch 190/1000, Train Loss: 0.0048, Test Loss: 0.0054\n",
      "Epoch 195/1000, Train Loss: 0.0047, Test Loss: 0.0053\n",
      "Epoch 200/1000, Train Loss: 0.0046, Test Loss: 0.0052\n",
      "Epoch 205/1000, Train Loss: 0.0045, Test Loss: 0.0052\n",
      "Epoch 210/1000, Train Loss: 0.0045, Test Loss: 0.0051\n",
      "Epoch 215/1000, Train Loss: 0.0044, Test Loss: 0.0050\n",
      "Epoch 220/1000, Train Loss: 0.0043, Test Loss: 0.0050\n",
      "Epoch 225/1000, Train Loss: 0.0042, Test Loss: 0.0049\n",
      "Epoch 230/1000, Train Loss: 0.0042, Test Loss: 0.0048\n",
      "Epoch 235/1000, Train Loss: 0.0041, Test Loss: 0.0048\n",
      "Epoch 240/1000, Train Loss: 0.0041, Test Loss: 0.0047\n",
      "Epoch 245/1000, Train Loss: 0.0040, Test Loss: 0.0046\n",
      "Epoch 250/1000, Train Loss: 0.0039, Test Loss: 0.0046\n",
      "Epoch 255/1000, Train Loss: 0.0039, Test Loss: 0.0045\n",
      "Epoch 260/1000, Train Loss: 0.0038, Test Loss: 0.0045\n",
      "Epoch 265/1000, Train Loss: 0.0038, Test Loss: 0.0044\n",
      "Epoch 270/1000, Train Loss: 0.0037, Test Loss: 0.0044\n",
      "Epoch 275/1000, Train Loss: 0.0037, Test Loss: 0.0043\n",
      "Epoch 280/1000, Train Loss: 0.0036, Test Loss: 0.0043\n",
      "Epoch 285/1000, Train Loss: 0.0036, Test Loss: 0.0043\n",
      "Epoch 290/1000, Train Loss: 0.0035, Test Loss: 0.0042\n",
      "Epoch 295/1000, Train Loss: 0.0035, Test Loss: 0.0042\n",
      "Epoch 300/1000, Train Loss: 0.0035, Test Loss: 0.0041\n",
      "Epoch 305/1000, Train Loss: 0.0034, Test Loss: 0.0041\n",
      "Epoch 310/1000, Train Loss: 0.0034, Test Loss: 0.0041\n",
      "Epoch 315/1000, Train Loss: 0.0033, Test Loss: 0.0040\n",
      "Epoch 320/1000, Train Loss: 0.0033, Test Loss: 0.0040\n",
      "Epoch 325/1000, Train Loss: 0.0033, Test Loss: 0.0040\n",
      "Epoch 330/1000, Train Loss: 0.0032, Test Loss: 0.0039\n",
      "Epoch 335/1000, Train Loss: 0.0032, Test Loss: 0.0039\n",
      "Epoch 340/1000, Train Loss: 0.0032, Test Loss: 0.0039\n",
      "Epoch 345/1000, Train Loss: 0.0031, Test Loss: 0.0038\n",
      "Epoch 350/1000, Train Loss: 0.0031, Test Loss: 0.0038\n",
      "Epoch 355/1000, Train Loss: 0.0031, Test Loss: 0.0038\n",
      "Epoch 360/1000, Train Loss: 0.0030, Test Loss: 0.0038\n",
      "Epoch 365/1000, Train Loss: 0.0030, Test Loss: 0.0037\n",
      "Epoch 370/1000, Train Loss: 0.0030, Test Loss: 0.0037\n",
      "Epoch 375/1000, Train Loss: 0.0030, Test Loss: 0.0037\n",
      "Epoch 380/1000, Train Loss: 0.0029, Test Loss: 0.0037\n",
      "Epoch 385/1000, Train Loss: 0.0029, Test Loss: 0.0036\n",
      "Epoch 390/1000, Train Loss: 0.0029, Test Loss: 0.0036\n",
      "Epoch 395/1000, Train Loss: 0.0029, Test Loss: 0.0036\n",
      "Epoch 400/1000, Train Loss: 0.0028, Test Loss: 0.0036\n",
      "Epoch 405/1000, Train Loss: 0.0028, Test Loss: 0.0035\n",
      "Epoch 410/1000, Train Loss: 0.0028, Test Loss: 0.0035\n",
      "Epoch 415/1000, Train Loss: 0.0028, Test Loss: 0.0035\n",
      "Epoch 420/1000, Train Loss: 0.0028, Test Loss: 0.0035\n",
      "Epoch 425/1000, Train Loss: 0.0027, Test Loss: 0.0035\n",
      "Epoch 430/1000, Train Loss: 0.0027, Test Loss: 0.0035\n",
      "Epoch 435/1000, Train Loss: 0.0027, Test Loss: 0.0034\n",
      "Epoch 440/1000, Train Loss: 0.0027, Test Loss: 0.0034\n",
      "Epoch 445/1000, Train Loss: 0.0027, Test Loss: 0.0034\n",
      "Epoch 450/1000, Train Loss: 0.0026, Test Loss: 0.0034\n",
      "Epoch 455/1000, Train Loss: 0.0026, Test Loss: 0.0034\n",
      "Epoch 460/1000, Train Loss: 0.0026, Test Loss: 0.0034\n",
      "Epoch 465/1000, Train Loss: 0.0026, Test Loss: 0.0033\n",
      "Epoch 470/1000, Train Loss: 0.0026, Test Loss: 0.0033\n",
      "Epoch 475/1000, Train Loss: 0.0026, Test Loss: 0.0033\n",
      "Epoch 480/1000, Train Loss: 0.0025, Test Loss: 0.0033\n",
      "Epoch 485/1000, Train Loss: 0.0025, Test Loss: 0.0033\n",
      "Epoch 490/1000, Train Loss: 0.0025, Test Loss: 0.0033\n",
      "Epoch 495/1000, Train Loss: 0.0025, Test Loss: 0.0033\n",
      "Epoch 500/1000, Train Loss: 0.0025, Test Loss: 0.0032\n",
      "Epoch 505/1000, Train Loss: 0.0025, Test Loss: 0.0032\n",
      "Epoch 510/1000, Train Loss: 0.0025, Test Loss: 0.0032\n",
      "Epoch 515/1000, Train Loss: 0.0024, Test Loss: 0.0032\n",
      "Epoch 520/1000, Train Loss: 0.0024, Test Loss: 0.0032\n",
      "Epoch 525/1000, Train Loss: 0.0024, Test Loss: 0.0032\n",
      "Epoch 530/1000, Train Loss: 0.0024, Test Loss: 0.0032\n",
      "Epoch 535/1000, Train Loss: 0.0024, Test Loss: 0.0032\n",
      "Epoch 540/1000, Train Loss: 0.0024, Test Loss: 0.0031\n",
      "Epoch 545/1000, Train Loss: 0.0024, Test Loss: 0.0031\n",
      "Epoch 550/1000, Train Loss: 0.0024, Test Loss: 0.0031\n",
      "Epoch 555/1000, Train Loss: 0.0023, Test Loss: 0.0031\n",
      "Epoch 560/1000, Train Loss: 0.0023, Test Loss: 0.0031\n",
      "Epoch 565/1000, Train Loss: 0.0023, Test Loss: 0.0031\n",
      "Epoch 570/1000, Train Loss: 0.0023, Test Loss: 0.0031\n",
      "Epoch 575/1000, Train Loss: 0.0023, Test Loss: 0.0031\n",
      "Epoch 580/1000, Train Loss: 0.0023, Test Loss: 0.0031\n",
      "Epoch 585/1000, Train Loss: 0.0023, Test Loss: 0.0031\n",
      "Epoch 590/1000, Train Loss: 0.0023, Test Loss: 0.0031\n",
      "Epoch 595/1000, Train Loss: 0.0023, Test Loss: 0.0030\n",
      "Epoch 600/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 605/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 610/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 615/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 620/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 625/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 630/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 635/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 640/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 645/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 650/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 655/1000, Train Loss: 0.0022, Test Loss: 0.0030\n",
      "Epoch 660/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 665/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 670/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 675/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 680/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 685/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 690/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 695/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 700/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 705/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 710/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 715/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 720/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 725/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 730/1000, Train Loss: 0.0021, Test Loss: 0.0029\n",
      "Epoch 735/1000, Train Loss: 0.0020, Test Loss: 0.0029\n",
      "Epoch 740/1000, Train Loss: 0.0020, Test Loss: 0.0029\n",
      "Epoch 745/1000, Train Loss: 0.0020, Test Loss: 0.0029\n",
      "Epoch 750/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 755/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 760/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 765/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 770/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 775/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 780/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 785/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 790/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 795/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 800/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 805/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 810/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 815/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 820/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 825/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 830/1000, Train Loss: 0.0020, Test Loss: 0.0028\n",
      "Epoch 835/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 840/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 845/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 850/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 855/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 860/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 865/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 870/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 875/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 880/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 885/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 890/1000, Train Loss: 0.0019, Test Loss: 0.0028\n",
      "Epoch 895/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 900/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 905/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 910/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 915/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 920/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 925/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 930/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 935/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 940/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 945/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 950/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 955/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 960/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 965/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 970/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 975/1000, Train Loss: 0.0019, Test Loss: 0.0027\n",
      "Epoch 980/1000, Train Loss: 0.0018, Test Loss: 0.0027\n",
      "Epoch 985/1000, Train Loss: 0.0018, Test Loss: 0.0027\n",
      "Epoch 990/1000, Train Loss: 0.0018, Test Loss: 0.0027\n",
      "Epoch 995/1000, Train Loss: 0.0018, Test Loss: 0.0027\n",
      "Epoch 1000/1000, Train Loss: 0.0018, Test Loss: 0.0027\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = full_gd(model,\n",
    "                                    criterion,\n",
    "                                    optimizer,\n",
    "                                    X_train,\n",
    "                                    y_train,\n",
    "                                    X_test,\n",
    "                                    y_test,\n",
    "                                    epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "NzD5qsMxJxuY",
    "outputId": "02733c51-d842-4404-b746-ad0087b12154"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the train loss and test loss per iteration\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "5IACnu3gJ4dK",
    "outputId": "b74b4960-2b57-4d9c-bdad-e19f86236cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.9998, Test acc: 0.9997\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy\n",
    "with torch.no_grad():\n",
    "  p_train = model(X_train)\n",
    "  p_train = (p_train.cpu().numpy() > 0)\n",
    "  train_acc = np.mean(y_train.cpu().numpy() == p_train)\n",
    "\n",
    "  p_test = model(X_test)\n",
    "  p_test = (p_test.cpu().numpy() > 0)\n",
    "  test_acc = np.mean(y_test.cpu().numpy() == p_test)\n",
    "print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PyTorch Stock Returns.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
